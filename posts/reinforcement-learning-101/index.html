<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Reinforcement Learning 101 | Mateusz Pieniak</title>
<meta name=keywords content="Reinforcement Learning,Bellman Optimality Equation,Bellman Expectation Equation,Banach Fixed-Point Theorem,Contraction Mapping,Value Iteration,Policy Iteration"><meta name=description content="Introduction to classical model-based Reinforcement Learning techniques."><meta name=author content="Mateusz Pieniak"><link rel=canonical href=https://elkotito.github.io/posts/reinforcement-learning-101/><link crossorigin=anonymous href=https://elkotito.github.io/assets/css/stylesheet.4f43e248382637b7d935f722075a52cc9533406ea39cc213db062b9b6418f6b6.css integrity="sha256-T0PiSDgmN7fZNfciB1pSzJUzQG6jnMIT2wYrm2QY9rY=" rel="preload stylesheet" as=style><link rel=icon href=https://elkotito.github.io/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://elkotito.github.io/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://elkotito.github.io/favicon-32x32.png><link rel=apple-touch-icon href=https://elkotito.github.io/apple-touch-icon.png><link rel=mask-icon href=https://elkotito.github.io/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://elkotito.github.io/posts/reinforcement-learning-101/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.22/dist/katex.min.css integrity=sha384-5TcZemv2l/9On385z///+d7MSYlvIEw9FuZTIdZ14vJLqWphw7e7ZPuOiCHJcFCP crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.22/dist/katex.min.js integrity=sha384-cMkvdD8LoxVzGF/RPUKAcvmm49FQ0oxwDF3BGKtDXcEc+T1b2N+teh/OJfpU0jr6 crossorigin=anonymous></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.22/dist/contrib/auto-render.min.js integrity=sha384-hCXGrW6PitJEwbkoStFjeJxv+fSOOQKOPbJxSfM6G5sWZjAyWhXiTIIAmQqnlLlh crossorigin=anonymous onload=renderMathInElement(document.body)></script>>
<script>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"\\[",right:"\\]",display:!0},{left:"$",right:"$",display:!1},{left:"\\(",right:"\\)",display:!1}]})})</script><meta property="og:url" content="https://elkotito.github.io/posts/reinforcement-learning-101/"><meta property="og:site_name" content="Mateusz Pieniak"><meta property="og:title" content="Reinforcement Learning 101"><meta property="og:description" content="Introduction to classical model-based Reinforcement Learning techniques."><meta property="og:locale" content="en-us"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2025-05-19T00:00:00+00:00"><meta property="article:modified_time" content="2025-05-19T00:00:00+00:00"><meta property="article:tag" content="Reinforcement Learning"><meta property="article:tag" content="Bellman Optimality Equation"><meta property="article:tag" content="Bellman Expectation Equation"><meta property="article:tag" content="Banach Fixed-Point Theorem"><meta property="article:tag" content="Contraction Mapping"><meta property="article:tag" content="Value Iteration"><meta name=twitter:card content="summary"><meta name=twitter:title content="Reinforcement Learning 101"><meta name=twitter:description content="Introduction to classical model-based Reinforcement Learning techniques."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://elkotito.github.io/posts/"},{"@type":"ListItem","position":2,"name":"Reinforcement Learning 101","item":"https://elkotito.github.io/posts/reinforcement-learning-101/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Reinforcement Learning 101","name":"Reinforcement Learning 101","description":"Introduction to classical model-based Reinforcement Learning techniques.","keywords":["Reinforcement Learning","Bellman Optimality Equation","Bellman Expectation Equation","Banach Fixed-Point Theorem","Contraction Mapping","Value Iteration","Policy Iteration"],"articleBody":"Introduction Imagine playing a computer game. In such a game you observe the screen and based on what you see on the screen you make a decision. With each decision you either get some points or not. Making a decision changes what you see on the screen. The same process we could describe in a more abstract mathematical language. In such a formal language, you are the agent.\nYou observe a state of the environment $s_t$ given a timestamp $t$. You execute an action $a_t$ by following your policy function $\\pi$. You get a reward $r_t$. The environment transitions to a new state $s_{t+1}$. The goal of reinforcement learning is to find an optimal policy function $\\pi^*$ that maximizes the total future rewards $G_t$. In other words, you want to find actions that let you get as many points as possible in the game.\n$$ G_t = R_{t} + R_{t+1} + R_{t+2} + \\ldots = \\sum_{k=0}^{\\infty} R_{t+k} $$\nHowever, with such a definition of $G_t$, the sum will be infinite which imposes certain mathematical complications. Having said that, for mathematical convenience, we modify and replace $G_t$ with discounted cumulative rewards where $0 \\leq \\gamma \\leq 1$ represents a discount factor.\n$$ G_t = R_{t} + \\gamma R_{t+1} + \\gamma^2 R_{t+2} + \\ldots = \\sum_{k=0}^{\\infty} \\gamma^k R_{t+k} = R_{t} + \\gamma G_{t+1} $$\nWhy is it finite? To illustrate that, we can assume a constant reward for each timestamp $R_t = R$, and we will end up with a finite geometric series.\n$$ G_t = R + \\gamma R + \\gamma^2 R + \\ldots = R \\sum_{k=0}^{\\infty} \\gamma^k = \\frac{R}{1-\\gamma} $$\nWhat mathematical framework should we use to model such a decision-making process? A natural choice is probability theory because usually our environment is not deterministic. It means that if we are in a state $s_t$, and we take an action $a_t$ we don’t always get the same reward $r_t$. Having said that, we can represent $S_t, R_t, A_t$ as random variables and the policy function $\\pi$ will be represented as a probability distribution over actions. Notice that $G_t$ will also be a random variable, hence now we would need to maximize the expected discounted cumulative reward.\n$$ \\mathbb{E}[G_t \\mid S_t = s_t, A_{t-1} = a_{t-1}, S_{t-1} = s_{t-1}, \\ldots, S_0 = s_0, A_0 = a_0] $$\nFor mathematical simplicity, we often assume such a process to be a Markov Decision Process (MDP) i.e. the next state depends only on the current state and action, not the whole history of states and actions.\n$$ p(s_{t+1} \\mid s_t, a_t, s_{t-1}, a_{t-1}, \\ldots, s_0, a_0) = p(s_{t+1} \\mid s_t, a_t) $$\nIt allows us to simplify the expected discounted cumulative reward and derive Bellman Equations in the next section.\n$$ \\mathbb{E}[G_t \\mid S_t = s] $$\nBellman Equations Value function We already know that the goal of reinforcement learning is to find an optimal policy $\\pi^* (s)$ that maximizes the expected discounted cumulative reward for every state $s$. In reinforcement learning, we define our objective as a value function $v_\\pi(s)$.\n$$ \\begin{aligned} v_\\pi(s) \u0026= \\mathbb{E_\\pi}[G_t \\mid S_t = s] \\qquad \\text{(recursive definition of } G_t \\text{)} \\\\[0.5em] \u0026= \\mathbb{E_\\pi}[R_t + \\gamma G_{t+1} \\mid S_t = s] \\qquad \\text{(linearity of expectation)} \\\\[0.5em] \u0026= \\mathbb{E_\\pi}[R_t \\mid S_t = s] + \\gamma \\mathbb{E_\\pi}[G_{t+1} \\mid S_t = s] \\qquad \\text{(law of total expectation)} \\\\[0.5em] \u0026= \\mathbb{E_\\pi}[R_t \\mid S_t = s] + \\gamma \\sum_{s^\\prime} p(s^\\prime \\mid s)\\mathbb{E_\\pi}[G_{t+1} \\mid S_t = s,S_{t+1} = s^\\prime] \\qquad \\text{(conditional independence)} \\\\[0.5em] \u0026= \\mathbb{E_\\pi}[R_t \\mid S_t = s] + \\gamma \\sum_{s^\\prime} p(s^\\prime \\mid s)\\mathbb{E_\\pi}[G_{t+1} \\mid S_{t+1} = s^\\prime] \\qquad \\text{(definition of } v_\\pi \\text{)} \\\\[0.5em] \u0026= \\mathbb{E_\\pi}[R_t \\mid S_t = s] + \\gamma \\sum_{s^\\prime} p(s^\\prime \\mid s) v_\\pi(s^\\prime) \\qquad \\text{(marginalization)} \\\\[0.5em] \u0026= \\mathbb{E_\\pi}[R_t \\mid S_t = s] + \\gamma \\sum_{r, a, s^\\prime} p(r, a, s^\\prime \\mid s) v_\\pi(s^\\prime) \\qquad \\text{(definition of } \\mathbb{E_\\pi} \\text{)} \\\\[0.5em] \u0026= \\sum_{r, a, s^\\prime} p(r, a, s^\\prime \\mid s) r + \\gamma \\sum_{r, a, s^\\prime} p(r, a, s^\\prime \\mid s) v_\\pi(s^\\prime) \\\\[0.5em] \u0026= \\sum_{r, a, s^\\prime} p(r, a, s^\\prime \\mid s) (r + \\gamma v_\\pi(s^\\prime)) \\qquad \\text{(probability chain rule)} \\\\[0.5em] \u0026= \\sum_{a}p (a \\mid s) \\sum_{r, s^\\prime} p(r, s^\\prime \\mid s, a) (r + \\gamma v_\\pi(s^\\prime)) \\\\[0.5em] \u0026= \\sum_{a}\\pi (a \\mid s) \\sum_{r, s^\\prime} p(r, s^\\prime \\mid s, a) (r + \\gamma v_\\pi(s^\\prime)) \\\\[0.5em] \\end{aligned} $$\nA bit of math, but the takeaway is that we can express $v_\\pi(s)$ as a function of itself i.e. $v_\\pi(s^\\prime)$, making it a recursive definition. We will see why it is useful once we derive the Value Iteration and Policy Iteration algorithms. Because $v(s)$ depends on the policy $\\pi(a \\mid s)$, we often write $v_\\pi(s).$\nAction-value function We defined the value function $v_\\pi(s)$ as the expected discounted cumulative reward in state $s$. We can introduce an additional useful function $q_\\pi(s, a)$ called the action-value function. The action-value function represents the expected discounted cumulative reward if an agent in state $s$ takes an action $a$ and keeps following policy $\\pi$ until the episode ends. We introduce it for mathematical convenience and ease of communication as it will appear in many equations.\n$$ q_\\pi(s, a) = \\mathbb{E_\\pi}[G_t \\mid S_t = s, A_t = a] = \\sum_{r, s^\\prime} p(r, s^\\prime \\mid s, a)(r + \\gamma v_\\pi(s^\\prime)) $$\nWe no longer need to sum over all actions $a$ since we condition our probability distribution on action $a$. Having defined the action-value function, we can use it to express the value function.\n$$ v_\\pi(s) = \\sum_{a}\\pi(a \\mid s) q_\\pi(s, a) $$\nBellman Expectation Equation A derived equation in the value function section is also known as the Bellman Expectation Equation. It is called “Expectation” because we sum over all actions which is equivalent to taking the expected value over the action-value function as you can see below.\n$$ v_\\pi(s) = \\sum_{a} \\pi(a \\mid s) \\sum_{r, s^\\prime} p(r, s^\\prime \\mid s, a) (r + \\gamma v_\\pi(s^\\prime)) = \\mathbb{E}_{a \\sim \\pi(\\cdot \\mid s)}[ q _{\\pi} (s, a) ] $$\nBellman Optimality Equation There is also the variant of the Bellman Expectation Equation known as the Bellman Optimality Equation where instead of taking expectation over all actions we act greedily and always take an action that maximizes the value function. We will see why it is useful later in this post.\n$$ v(s) = \\max_{a} \\sum_{r, s^\\prime} p(r, s^\\prime \\mid s, a) (r + \\gamma v(s^\\prime)) = \\max_{a} q (s, a) \\\\[0.5em] $$\nUnlike the Bellman Expectation Equation, notice that our value function $v(s)$ doesn’t depend on the policy $\\pi$, hence we used $v(s)$ instead of $v_\\pi(s)$ on purpose. This observation is a foundation of the off-policy algorithms in reinforcement learning (e.g. Q-Learning) as you will learn later.\nContraction Mapping You might wonder why we introduced the Bellman Expectation Equation and the Bellman Optimality Equation in the first place. It will become obvious once we switch our focus a bit from reinforcement learning and understand contraction mapping first. This part of math will be useful to derive algorithms to find the optimal policy.\nDefinition A contraction mapping is a function $T: X \\rightarrow X$ that maps a metric space $X$ onto itself with the property that the distance $d$ between any two points $x$ and $y$ in the space is reduced after applying the function $T$. Such a reduction in distance we can express with a constant $0 \\leq \\kappa \u003c 1$ such that:\n$$d(T(x), T(y)) \\leq \\kappa \\cdot d(x, y)$$\nIn other words, applying the function $T$ brings points closer together by at least a factor of $\\kappa$.\nBanach Fixed-Point Theorem The Banach Fixed-Point Theorem states that if $T$ is a contraction mapping then:\n$T$ has a unique fixed point $x^*$ such that $T(x^ *) = x^ *$. For any initial point $x_0$ in the space, the sequence $x_0, T(x_0), T(T(x_0)), \\ldots$ converges to that fixed point $x^*$. In other words, if we keep applying the function $T$ starting with some initial point $x_0$ it will converge to the point $x^*$.\nApplications in RL Now you probably wonder why is it useful in reinforcement learning? Imagine for a second that any point $x$, $y$, $\\ldots$ in the metric space is a function, not a real number as you are probably used to think at school. The takeaway from the Banach Fixed-Point Theorem is that if we found a function $T$ that is a contraction mapping, then we would have an iterative algorithm to find the true function $x$, beginning from any function $x_0$.\nWell, it seems like there is a proof that the Bellman Expectation Equation and the Bellman Optimality Equations are contraction mappings! Notice that we couldn’t even think about proving such a fact without the Bellman Equations, because the recursive formulation of these equations allows us to use it as an operator $T$ as it maps a metric space onto itself $T: X \\rightarrow X$.\nFor the Bellman Expectation Equation, the operator $T^\\pi$ is defined as:\n$$T^\\pi (v(s)) = \\sum_{a} \\pi(a \\mid s) \\sum_{r, s^\\prime} p(r, s^\\prime \\mid s, a) (r + \\gamma v_\\pi(s^\\prime))$$\nFor the Bellman Optimality Equation, the operator $T^*$ is defined as:\n$$T^* (v(s)) = \\max_{a} \\sum_{r, s^\\prime} p(r, s^\\prime \\mid s, a) (r + \\gamma v(s^\\prime))$$\nBoth of these operators directly lead to two common algorithms. The contraction property ensures that:\nIn Value Iteration, repeatedly applying $T^*$ will converge to the optimal value function $v^ *$ In Policy Iteration, repeatedly applying $T^\\pi$ for a fixed policy $\\pi$ will converge to the value function $v_\\pi$ for that policy This mathematical foundation is why we can be confident that these iterative algorithms will eventually find the solutions we’re looking for.\nAlgorithms Below you can find two common algorithms that allow us to find the optimal policy $\\pi^*(s)$.\nValue Iteration As we’ve seen, the Bellman Optimality Equation is a contraction mapping. This means we can apply it iteratively to compute the optimal value function $v^* (s)$.\nIteratively apply the Bellman Optimality Equation for a value function until it converges to the optimal value function $v^*(s)$. $$ v(s) = \\max_{a} \\sum_{r, s^\\prime} p(r, s^\\prime \\mid s, a) (r + \\gamma v(s^\\prime)) $$ How do we know when convergence happens? We need to observe both $v_t(s)$ and $v_{t+1}(s)$ until they stop changing by a small predefined value $\\epsilon$.\nOnce we computed the optimal value function $v^* (s)$, we can extract the corresponding optimal action-value function $q^* (s, a)$ and the optimal policy $\\pi^* (s)$ in a single step, using the equations we already know. $$ q^* (s, a) = \\sum_{r, s^\\prime} p(r, s^\\prime \\mid s, a)(r + \\gamma v^* (s^\\prime)) \\\\[0.5em] \\pi^* (s) = \\argmax_{a}{q^*(s, a)} $$ We can extract the optimal policy $\\pi^ * (s)$ because the optimal policy is the one that maximizes the value function for each state $s$.\nWhat is interesting about the Value Iteration algorithm is that we never update a policy during the algorithm. We only update our value function $v(s)$. Any intermediate value function $v(s)$ that is not optimal might not correspond to any reasonable policy $\\pi(s)$.\nPolicy Iteration As we’ve established, the Bellman Expectation Equation is a contraction mapping. This allows us to apply it recursively to compute the value function $v_\\pi(s)$ for our policy $\\pi(s)$.\nWhat we compute in Policy Iteration is not an optimal value function $v^* (s)$ as in the Value Iteration algorithm, but a value function $v_\\pi(s)$ that corresponds to a policy $\\pi(s)$. Now having computed $v_\\pi (s)$ given policy $\\pi$, how do we find the optimal policy? We will use the Policy Improvement Theorem.\nPolicy Improvement Theorem The theorem states that acting greedily with respect to $q_\\pi(s, a)$ i.e. extracting our new policy $\\pi^\\prime(s) = \\argmax_a q_\\pi(s, a)$ improves or doesn’t change our current policy $\\pi(s)$. It is true because of the following inequality as the $\\max_a$ function will always be equal to or higher than a weighted average of $q_\\pi(s, a)$.\n$$ v_\\pi(s) = \\sum_a \\pi(a \\mid s) q_\\pi(s, a) \\leq \\max_a q_\\pi(s, a) = v_{\\pi^\\prime}(s) $$\nImproving a policy means that our value function is better for every state $s$. In other words, we can expect a higher cumulative discounted reward as $v_\\pi(s) \\leq v_{\\pi^\\prime}(s)$.\nAlgorithm Iteratively apply the Bellman Expectation Equation for a value function until it converges to the value function $v_\\pi (s)$ for policy $\\pi$. $$ v_\\pi(s) = \\sum_{a} \\pi(a \\mid s) \\sum_{r, s^\\prime} p(r, s^\\prime \\mid s, a) (r + \\gamma v_{\\pi}(s^\\prime)) $$\nUpdate the policy $\\pi(s)$ by acting greedily with respect to $q_\\pi(s, a)$ to obtain a new policy $\\pi^*$. $$\\pi^\\prime(s) = \\argmax_a q_\\pi(s, a)$$\nRepeat until the policy stops changing for each state $s$ i.e. $|\\pi_{t+1}(s) - \\pi_{t}(s)| \\leq \\epsilon $.\nExample Let’s play a game. We are given a grid world in which each cell represents a state $s$. We can take 4 actions (up, down, left, right). We win the game when we obtain a gift in the shortest time possible while avoiding falling down. We can model the shortest possible time as assigning $-1$ rewards with each timestep.\nCode You can find my naive implementation of these algorithms on my GitHub:\nhttps://github.com/elkotito/rl-tutorial/blob/main/model_based_algorithms.py\nPolicy You can see an optimal policy on the image below:\nFinal thoughts In this article we discussed algorithms to learn the agent’s behavior expressed as policy $\\pi$ in model-based environments. A model-based environment means that we have direct access to the environment’s dynamics i.e. $p(r, s^\\prime \\mid s, a)$ probabilities. As you can tell, it is rarely the case in the real world. In the next article we will discuss model-free environments which is a situation when you can rely only on data collected by interacting with environment.\n","wordCount":"2257","inLanguage":"en","datePublished":"2025-05-19T00:00:00Z","dateModified":"2025-05-19T00:00:00Z","author":{"@type":"Person","name":"Mateusz Pieniak"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://elkotito.github.io/posts/reinforcement-learning-101/"},"publisher":{"@type":"Organization","name":"Mateusz Pieniak","logo":{"@type":"ImageObject","url":"https://elkotito.github.io/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://elkotito.github.io/ accesskey=h title="Mateusz Pieniak (Alt + H)">Mateusz Pieniak</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)" aria-label="Toggle theme"><svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://elkotito.github.io/ title=Posts><span>Posts</span></a></li><li><a href=https://elkotito.github.io/search/ title="Search (Alt + /)" accesskey=/><span>Search</span></a></li><li><a href=https://elkotito.github.io/tags/ title=Tags><span>Tags</span></a></li><li><a href=https://elkotito.github.io/index.xml title=RSS><span>RSS</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><h1 class="post-title entry-hint-parent">Reinforcement Learning 101</h1><div class=post-description>Introduction to classical model-based Reinforcement Learning techniques.</div><div class=post-meta><span title='2025-05-19 00:00:00 +0000 UTC'>May 19, 2025</span>&nbsp;·&nbsp;11 min&nbsp;·&nbsp;Mateusz Pieniak</div></header><div class=toc><details open><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><ul><li><a href=#introduction aria-label=Introduction>Introduction</a></li><li><a href=#bellman-equations aria-label="Bellman Equations">Bellman Equations</a><ul><li><a href=#value-function aria-label="Value function">Value function</a></li><li><a href=#action-value-function aria-label="Action-value function">Action-value function</a></li><li><a href=#bellman-expectation-equation aria-label="Bellman Expectation Equation">Bellman Expectation Equation</a></li><li><a href=#bellman-optimality-equation aria-label="Bellman Optimality Equation">Bellman Optimality Equation</a></li></ul></li><li><a href=#contraction-mapping aria-label="Contraction Mapping">Contraction Mapping</a><ul><li><a href=#definition aria-label=Definition>Definition</a></li><li><a href=#banach-fixed-point-theorem aria-label="Banach Fixed-Point Theorem">Banach Fixed-Point Theorem</a></li><li><a href=#applications-in-rl aria-label="Applications in RL">Applications in RL</a></li></ul></li><li><a href=#algorithms aria-label=Algorithms>Algorithms</a><ul><li><a href=#value-iteration aria-label="Value Iteration">Value Iteration</a></li><li><a href=#policy-iteration aria-label="Policy Iteration">Policy Iteration</a><ul><li><a href=#policy-improvement-theorem aria-label="Policy Improvement Theorem">Policy Improvement Theorem</a></li><li><a href=#algorithm aria-label=Algorithm>Algorithm</a></li></ul></li></ul></li><li><a href=#example aria-label=Example>Example</a><ul><li><a href=#code aria-label=Code>Code</a></li><li><a href=#policy aria-label=Policy>Policy</a></li></ul></li><li><a href=#final-thoughts aria-label="Final thoughts">Final thoughts</a></li></ul></div></details></div><div class=post-content><h1 id=introduction>Introduction<a hidden class=anchor aria-hidden=true href=#introduction>#</a></h1><p>Imagine playing a computer game. In such a game you observe the screen and based on what you see on the screen you make a decision.
With each decision you either get some points or not. Making a decision changes what you see on the screen.
The same process we could describe in a more abstract mathematical language. In such a formal language, you are the agent.</p><ul><li>You observe a state of the environment $s_t$ given a timestamp $t$.</li><li>You execute an action $a_t$ by following your policy function $\pi$.</li><li>You get a reward $r_t$.</li><li>The environment transitions to a new state $s_{t+1}$.</li></ul><p>The goal of reinforcement learning is to find an optimal policy function $\pi^*$ that maximizes the total future rewards $G_t$. In other words, you want to find actions that let you get as many points as possible in the game.</p><p>$$
G_t = R_{t} + R_{t+1} + R_{t+2} + \ldots = \sum_{k=0}^{\infty} R_{t+k}
$$</p><p>However, with such a definition of $G_t$, the sum will be infinite which imposes certain mathematical complications.
Having said that, for mathematical convenience, we modify and replace $G_t$ with discounted cumulative rewards where $0 \leq \gamma \leq 1$ represents a discount factor.</p><p>$$
G_t = R_{t} + \gamma R_{t+1} + \gamma^2 R_{t+2} + \ldots = \sum_{k=0}^{\infty} \gamma^k R_{t+k} = R_{t} + \gamma G_{t+1}
$$</p><p>Why is it finite? To illustrate that, we can assume a constant reward for each timestamp $R_t = R$, and we will end up with a finite geometric series.</p><p>$$
G_t = R + \gamma R + \gamma^2 R + \ldots = R \sum_{k=0}^{\infty} \gamma^k = \frac{R}{1-\gamma}
$$</p><p>What mathematical framework should we use to model such a decision-making process? A natural choice is probability theory because
usually our environment is not deterministic. It means that if we are in a state $s_t$, and we take an action $a_t$ we don&rsquo;t always get the same reward $r_t$.
Having said that, we can represent $S_t, R_t, A_t$ as random variables and the policy function $\pi$ will be represented as a probability distribution over actions.
Notice that $G_t$ will also be a random variable, hence now we would need to maximize the expected discounted cumulative reward.</p><p>$$
\mathbb{E}[G_t \mid S_t = s_t, A_{t-1} = a_{t-1}, S_{t-1} = s_{t-1}, \ldots, S_0 = s_0, A_0 = a_0]
$$</p><p>For mathematical simplicity, we often assume such a process to be a Markov Decision Process (MDP) i.e. the next state depends only on the current state and action, not the whole history of states and actions.</p><p>$$
p(s_{t+1} \mid s_t, a_t, s_{t-1}, a_{t-1}, \ldots, s_0, a_0) = p(s_{t+1} \mid s_t, a_t)
$$</p><p>It allows us to simplify the expected discounted cumulative reward and derive Bellman Equations in the next section.</p><p>$$
\mathbb{E}[G_t \mid S_t = s]
$$</p><h1 id=bellman-equations>Bellman Equations<a hidden class=anchor aria-hidden=true href=#bellman-equations>#</a></h1><h2 id=value-function>Value function<a hidden class=anchor aria-hidden=true href=#value-function>#</a></h2><p>We already know that the goal of reinforcement learning is to find an optimal policy $\pi^* (s)$ that maximizes the expected discounted cumulative reward for every state $s$.
In reinforcement learning, we define our objective as a value function $v_\pi(s)$.</p><p>$$
\begin{aligned}
v_\pi(s) &= \mathbb{E_\pi}[G_t \mid S_t = s] \qquad \text{(recursive definition of } G_t \text{)} \\[0.5em]
&= \mathbb{E_\pi}[R_t + \gamma G_{t+1} \mid S_t = s] \qquad \text{(linearity of expectation)} \\[0.5em]
&= \mathbb{E_\pi}[R_t \mid S_t = s] + \gamma \mathbb{E_\pi}[G_{t+1} \mid S_t = s] \qquad \text{(law of total expectation)} \\[0.5em]
&= \mathbb{E_\pi}[R_t \mid S_t = s] + \gamma \sum_{s^\prime} p(s^\prime \mid s)\mathbb{E_\pi}[G_{t+1} \mid S_t = s,S_{t+1} = s^\prime] \qquad \text{(conditional independence)} \\[0.5em]
&= \mathbb{E_\pi}[R_t \mid S_t = s] + \gamma \sum_{s^\prime} p(s^\prime \mid s)\mathbb{E_\pi}[G_{t+1} \mid S_{t+1} = s^\prime] \qquad \text{(definition of } v_\pi \text{)} \\[0.5em]
&= \mathbb{E_\pi}[R_t \mid S_t = s] + \gamma \sum_{s^\prime} p(s^\prime \mid s) v_\pi(s^\prime) \qquad \text{(marginalization)} \\[0.5em]
&= \mathbb{E_\pi}[R_t \mid S_t = s] + \gamma \sum_{r, a, s^\prime} p(r, a, s^\prime \mid s) v_\pi(s^\prime) \qquad \text{(definition of } \mathbb{E_\pi} \text{)} \\[0.5em]
&= \sum_{r, a, s^\prime} p(r, a, s^\prime \mid s) r + \gamma \sum_{r, a, s^\prime} p(r, a, s^\prime \mid s) v_\pi(s^\prime) \\[0.5em]
&= \sum_{r, a, s^\prime} p(r, a, s^\prime \mid s) (r + \gamma v_\pi(s^\prime)) \qquad \text{(probability chain rule)} \\[0.5em]
&= \sum_{a}p (a \mid s) \sum_{r, s^\prime} p(r, s^\prime \mid s, a) (r + \gamma v_\pi(s^\prime)) \\[0.5em]
&= \sum_{a}\pi (a \mid s) \sum_{r, s^\prime} p(r, s^\prime \mid s, a) (r + \gamma v_\pi(s^\prime)) \\[0.5em]
\end{aligned}
$$</p><p>A bit of math, but the takeaway is that we can express $v_\pi(s)$ as a function of itself i.e. $v_\pi(s^\prime)$, making it a recursive definition. We will see why it is useful once we derive the Value Iteration and Policy Iteration algorithms.
Because $v(s)$ depends on the policy $\pi(a \mid s)$, we often write $v_\pi(s).$</p><h2 id=action-value-function>Action-value function<a hidden class=anchor aria-hidden=true href=#action-value-function>#</a></h2><p>We defined the value function $v_\pi(s)$ as the expected discounted cumulative reward in state $s$. We can introduce an additional useful function $q_\pi(s, a)$ called the action-value function.
The action-value function represents the expected discounted cumulative reward if an agent in state $s$ takes an action $a$ and keeps following policy $\pi$ until the episode ends.
We introduce it for mathematical convenience and ease of communication as it will appear in many equations.</p><p>$$
q_\pi(s, a) = \mathbb{E_\pi}[G_t \mid S_t = s, A_t = a] = \sum_{r, s^\prime} p(r, s^\prime \mid s, a)(r + \gamma v_\pi(s^\prime))
$$</p><p>We no longer need to sum over all actions $a$ since we condition our probability distribution on action $a$.
Having defined the action-value function, we can use it to express the value function.</p><p>$$
v_\pi(s) = \sum_{a}\pi(a \mid s) q_\pi(s, a)
$$</p><h2 id=bellman-expectation-equation>Bellman Expectation Equation<a hidden class=anchor aria-hidden=true href=#bellman-expectation-equation>#</a></h2><p>A derived equation in the value function section is also known as the Bellman Expectation Equation. It is called &ldquo;Expectation&rdquo; because we sum over all actions which is equivalent to taking the expected value over the action-value function as you can see below.</p><p>$$
v_\pi(s) = \sum_{a} \pi(a \mid s) \sum_{r, s^\prime} p(r, s^\prime \mid s, a) (r + \gamma v_\pi(s^\prime)) = \mathbb{E}_{a \sim \pi(\cdot \mid s)}[ q _{\pi} (s, a) ]
$$</p><h2 id=bellman-optimality-equation>Bellman Optimality Equation<a hidden class=anchor aria-hidden=true href=#bellman-optimality-equation>#</a></h2><p>There is also the variant of the Bellman Expectation Equation known as the Bellman Optimality Equation where instead of taking expectation over all actions we act greedily and always take an action that maximizes the value function. We will see why it is useful later in this post.</p><p>$$
v(s) = \max_{a} \sum_{r, s^\prime} p(r, s^\prime \mid s, a) (r + \gamma v(s^\prime)) = \max_{a} q (s, a) \\[0.5em]
$$</p><p>Unlike the Bellman Expectation Equation, notice that our value function $v(s)$ doesn&rsquo;t depend on the policy $\pi$, hence we used $v(s)$ instead of $v_\pi(s)$ on purpose.
This observation is a foundation of the off-policy algorithms in reinforcement learning (e.g. Q-Learning) as you will learn later.</p><h1 id=contraction-mapping>Contraction Mapping<a hidden class=anchor aria-hidden=true href=#contraction-mapping>#</a></h1><p>You might wonder why we introduced the Bellman Expectation Equation and the Bellman Optimality Equation in the first place.
It will become obvious once we switch our focus a bit from reinforcement learning and understand contraction mapping first.
This part of math will be useful to derive algorithms to find the optimal policy.</p><h2 id=definition>Definition<a hidden class=anchor aria-hidden=true href=#definition>#</a></h2><p>A contraction mapping is a function $T: X \rightarrow X$ that maps a metric space $X$ onto itself with the property that the distance $d$ between any two points $x$ and $y$ in the space is reduced after applying the function $T$.
Such a reduction in distance we can express with a constant $0 \leq \kappa &lt; 1$ such that:</p><p>$$d(T(x), T(y)) \leq \kappa \cdot d(x, y)$$</p><p>In other words, applying the function $T$ brings points closer together by at least a factor of $\kappa$.</p><h2 id=banach-fixed-point-theorem>Banach Fixed-Point Theorem<a hidden class=anchor aria-hidden=true href=#banach-fixed-point-theorem>#</a></h2><p>The Banach Fixed-Point Theorem states that if $T$ is a contraction mapping then:</p><ol><li>$T$ has a unique fixed point $x^*$ such that $T(x^ *) = x^ *$.</li><li>For any initial point $x_0$ in the space, the sequence $x_0, T(x_0), T(T(x_0)), \ldots$ converges to that fixed point $x^*$.</li></ol><p>In other words, if we keep applying the function $T$ starting with some initial point $x_0$ it will converge to the point $x^*$.</p><h2 id=applications-in-rl>Applications in RL<a hidden class=anchor aria-hidden=true href=#applications-in-rl>#</a></h2><p>Now you probably wonder why is it useful in reinforcement learning?
Imagine for a second that any point $x$, $y$, $\ldots$ in the metric space is a function, not a real number as you are probably used to think at school.
The takeaway from the Banach Fixed-Point Theorem is that if we found a function $T$ that is a contraction mapping, then we would have an iterative algorithm to find the true function $x$, beginning from any function $x_0$.</p><p>Well, it seems like there is a proof that the Bellman Expectation Equation and the Bellman Optimality Equations are contraction mappings!
Notice that we couldn&rsquo;t even think about proving such a fact without the Bellman Equations, because the recursive formulation of these equations allows us to use it as an operator $T$ as it maps a metric space onto itself $T: X \rightarrow X$.</p><p>For the Bellman Expectation Equation, the operator $T^\pi$ is defined as:</p><p>$$T^\pi (v(s)) = \sum_{a} \pi(a \mid s) \sum_{r, s^\prime} p(r, s^\prime \mid s, a) (r + \gamma v_\pi(s^\prime))$$</p><p>For the Bellman Optimality Equation, the operator $T^*$ is defined as:</p><p>$$T^* (v(s)) = \max_{a} \sum_{r, s^\prime} p(r, s^\prime \mid s, a) (r + \gamma v(s^\prime))$$</p><p>Both of these operators directly lead to two common algorithms. The contraction property ensures that:</p><ol><li>In Value Iteration, repeatedly applying $T^*$ will converge to the optimal value function $v^ *$</li><li>In Policy Iteration, repeatedly applying $T^\pi$ for a fixed policy $\pi$ will converge to the value function $v_\pi$ for that policy</li></ol><p>This mathematical foundation is why we can be confident that these iterative algorithms will eventually find the solutions we&rsquo;re looking for.</p><h1 id=algorithms>Algorithms<a hidden class=anchor aria-hidden=true href=#algorithms>#</a></h1><p>Below you can find two common algorithms that allow us to find the optimal policy $\pi^*(s)$.</p><h2 id=value-iteration>Value Iteration<a hidden class=anchor aria-hidden=true href=#value-iteration>#</a></h2><p>As we&rsquo;ve seen, the Bellman Optimality Equation is a contraction mapping. This means we can apply it iteratively to compute the optimal value function $v^* (s)$.</p><ol><li><p>Iteratively apply the Bellman Optimality Equation for a value function until it converges to the optimal value function $v^*(s)$.
$$
v(s) = \max_{a} \sum_{r, s^\prime} p(r, s^\prime \mid s, a) (r + \gamma v(s^\prime))
$$
How do we know when convergence happens? We need to observe both $v_t(s)$ and $v_{t+1}(s)$ until they stop changing by a small predefined value $\epsilon$.</p></li><li><p>Once we computed the optimal value function $v^* (s)$, we can extract the corresponding optimal action-value function $q^* (s, a)$ and the optimal policy $\pi^* (s)$ in a single step, using the equations we already know.
$$
q^* (s, a) = \sum_{r, s^\prime} p(r, s^\prime \mid s, a)(r + \gamma v^* (s^\prime)) \\[0.5em]
\pi^* (s) = \argmax_{a}{q^*(s, a)}
$$
We can extract the optimal policy $\pi^ * (s)$ because the optimal policy is the one that maximizes the value function for each state $s$.</p></li></ol><p>What is interesting about the Value Iteration algorithm is that we never update a policy during the algorithm.
We only update our value function $v(s)$. Any intermediate value function $v(s)$ that is not optimal might not correspond to any reasonable policy $\pi(s)$.</p><h2 id=policy-iteration>Policy Iteration<a hidden class=anchor aria-hidden=true href=#policy-iteration>#</a></h2><p>As we&rsquo;ve established, the Bellman Expectation Equation is a contraction mapping. This allows us to apply it recursively to compute the value function $v_\pi(s)$ for our policy $\pi(s)$.</p><p>What we compute in Policy Iteration is not an optimal value function $v^* (s)$ as in the Value Iteration algorithm, but a value function $v_\pi(s)$ that corresponds to a policy $\pi(s)$.
Now having computed $v_\pi (s)$ given policy $\pi$, how do we find the optimal policy? We will use the Policy Improvement Theorem.</p><h3 id=policy-improvement-theorem>Policy Improvement Theorem<a hidden class=anchor aria-hidden=true href=#policy-improvement-theorem>#</a></h3><p>The theorem states that acting greedily with respect to $q_\pi(s, a)$ i.e. extracting our new policy $\pi^\prime(s) = \argmax_a q_\pi(s, a)$ improves or doesn&rsquo;t change our current policy $\pi(s)$.
It is true because of the following inequality as the $\max_a$ function will always be equal to or higher than a weighted average of $q_\pi(s, a)$.</p><p>$$
v_\pi(s) = \sum_a \pi(a \mid s) q_\pi(s, a) \leq \max_a q_\pi(s, a) = v_{\pi^\prime}(s)
$$</p><p>Improving a policy means that our value function is better for every state $s$. In other words, we can expect a higher cumulative discounted reward as $v_\pi(s) \leq v_{\pi^\prime}(s)$.</p><h3 id=algorithm>Algorithm<a hidden class=anchor aria-hidden=true href=#algorithm>#</a></h3><ol><li><p>Iteratively apply the Bellman Expectation Equation for a value function until it converges to the value function $v_\pi (s)$ for policy $\pi$.
$$
v_\pi(s) = \sum_{a} \pi(a \mid s) \sum_{r, s^\prime} p(r, s^\prime \mid s, a) (r + \gamma v_{\pi}(s^\prime))
$$</p></li><li><p>Update the policy $\pi(s)$ by acting greedily with respect to $q_\pi(s, a)$ to obtain a new policy $\pi^*$.
$$\pi^\prime(s) = \argmax_a q_\pi(s, a)$$</p></li><li><p>Repeat until the policy stops changing for each state $s$ i.e. $|\pi_{t+1}(s) - \pi_{t}(s)| \leq \epsilon $.</p></li></ol><h1 id=example>Example<a hidden class=anchor aria-hidden=true href=#example>#</a></h1><p>Let&rsquo;s play a game. We are given a grid world in which each cell represents a state $s$.
We can take 4 actions (up, down, left, right). We win the game when we obtain a gift in the shortest time possible while avoiding falling down.
We can model the shortest possible time as assigning $-1$ rewards with each timestep.</p><p><img alt="Frozen Lake empty" loading=lazy src=https://elkotito.github.io/posts/reinforcement-learning-101/frozen_lake_empty.png></p><h2 id=code>Code<a hidden class=anchor aria-hidden=true href=#code>#</a></h2><p>You can find my naive implementation of these algorithms on my GitHub:</p><p><a href=https://github.com/elkotito/rl-tutorial/blob/main/model_based_algorithms.py>https://github.com/elkotito/rl-tutorial/blob/main/model_based_algorithms.py</a></p><h2 id=policy>Policy<a hidden class=anchor aria-hidden=true href=#policy>#</a></h2><p>You can see an optimal policy on the image below:</p><p><img alt="Frozen Lake policy" loading=lazy src=https://elkotito.github.io/posts/reinforcement-learning-101/frozen_lake_policy.png></p><h1 id=final-thoughts>Final thoughts<a hidden class=anchor aria-hidden=true href=#final-thoughts>#</a></h1><p>In this article we discussed algorithms to learn the agent&rsquo;s behavior expressed as policy $\pi$ in model-based environments. A model-based environment means that we have direct access to the environment&rsquo;s dynamics i.e. $p(r, s^\prime \mid s, a)$ probabilities.
As you can tell, it is rarely the case in the real world. In the next article we will discuss model-free environments which is a situation when you can rely only on data collected by interacting with environment.</p></div><footer class=post-footer><ul class=post-tags><li><a href=https://elkotito.github.io/tags/reinforcement-learning/>Reinforcement Learning</a></li><li><a href=https://elkotito.github.io/tags/bellman-optimality-equation/>Bellman Optimality Equation</a></li><li><a href=https://elkotito.github.io/tags/bellman-expectation-equation/>Bellman Expectation Equation</a></li><li><a href=https://elkotito.github.io/tags/banach-fixed-point-theorem/>Banach Fixed-Point Theorem</a></li><li><a href=https://elkotito.github.io/tags/contraction-mapping/>Contraction Mapping</a></li><li><a href=https://elkotito.github.io/tags/value-iteration/>Value Iteration</a></li><li><a href=https://elkotito.github.io/tags/policy-iteration/>Policy Iteration</a></li></ul><ul class=share-buttons><li><a target=_blank rel="noopener noreferrer" aria-label="share Reinforcement Learning 101 on x" href="https://x.com/intent/tweet/?text=Reinforcement%20Learning%20101&amp;url=https%3a%2f%2felkotito.github.io%2fposts%2freinforcement-learning-101%2f&amp;hashtags=ReinforcementLearning%2cBellmanOptimalityEquation%2cBellmanExpectationEquation%2cBanachFixed-PointTheorem%2cContractionMapping%2cValueIteration%2cPolicyIteration"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446C483.971.0 512 28.03 512 62.554zM269.951 190.75 182.567 75.216H56L207.216 272.95 63.9 436.783h61.366L235.9 310.383l96.667 126.4H456L298.367 228.367l134-153.151H371.033zM127.633 110h36.468l219.38 290.065H349.5z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Reinforcement Learning 101 on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2felkotito.github.io%2fposts%2freinforcement-learning-101%2f&amp;title=Reinforcement%20Learning%20101&amp;summary=Reinforcement%20Learning%20101&amp;source=https%3a%2f%2felkotito.github.io%2fposts%2freinforcement-learning-101%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Reinforcement Learning 101 on reddit" href="https://reddit.com/submit?url=https%3a%2f%2felkotito.github.io%2fposts%2freinforcement-learning-101%2f&title=Reinforcement%20Learning%20101"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zM326.526 373.831c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zM160.91 295.348c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Reinforcement Learning 101 on facebook" href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2felkotito.github.io%2fposts%2freinforcement-learning-101%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978V319.085h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Reinforcement Learning 101 on whatsapp" href="https://api.whatsapp.com/send?text=Reinforcement%20Learning%20101%20-%20https%3a%2f%2felkotito.github.io%2fposts%2freinforcement-learning-101%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zm-58.673 127.703c-33.842-33.881-78.847-52.548-126.798-52.568-98.799.0-179.21 80.405-179.249 179.234-.013 31.593 8.241 62.428 23.927 89.612l-25.429 92.884 95.021-24.925c26.181 14.28 55.659 21.807 85.658 21.816h.074c98.789.0 179.206-80.413 179.247-179.243.018-47.895-18.61-92.93-52.451-126.81zM263.976 403.485h-.06c-26.734-.01-52.954-7.193-75.828-20.767l-5.441-3.229-56.386 14.792 15.05-54.977-3.542-5.637c-14.913-23.72-22.791-51.136-22.779-79.287.033-82.142 66.867-148.971 149.046-148.971 39.793.014 77.199 15.531 105.329 43.692 28.128 28.16 43.609 65.592 43.594 105.4-.034 82.149-66.866 148.983-148.983 148.984zm81.721-111.581c-4.479-2.242-26.499-13.075-30.604-14.571-4.105-1.495-7.091-2.241-10.077 2.241-2.986 4.483-11.569 14.572-14.182 17.562-2.612 2.988-5.225 3.364-9.703 1.12-4.479-2.241-18.91-6.97-36.017-22.23C231.8 264.15 222.81 249.484 220.198 245s-.279-6.908 1.963-9.14c2.016-2.007 4.48-5.232 6.719-7.847 2.24-2.615 2.986-4.484 4.479-7.472 1.493-2.99.747-5.604-.374-7.846-1.119-2.241-10.077-24.288-13.809-33.256-3.635-8.733-7.327-7.55-10.077-7.688-2.609-.13-5.598-.158-8.583-.158-2.986.0-7.839 1.121-11.944 5.604-4.105 4.484-15.675 15.32-15.675 37.364.0 22.046 16.048 43.342 18.287 46.332 2.24 2.99 31.582 48.227 76.511 67.627 10.685 4.615 19.028 7.371 25.533 9.434 10.728 3.41 20.492 2.929 28.209 1.775 8.605-1.285 26.499-10.833 30.231-21.295 3.732-10.464 3.732-19.431 2.612-21.298-1.119-1.869-4.105-2.99-8.583-5.232z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Reinforcement Learning 101 on telegram" href="https://telegram.me/share/url?text=Reinforcement%20Learning%20101&amp;url=https%3a%2f%2felkotito.github.io%2fposts%2freinforcement-learning-101%2f"><svg viewBox="2 2 28 28" height="30" width="30" fill="currentcolor"><path d="M26.49 29.86H5.5a3.37 3.37.0 01-2.47-1 3.35 3.35.0 01-1-2.47V5.48A3.36 3.36.0 013 3 3.37 3.37.0 015.5 2h21A3.38 3.38.0 0129 3a3.36 3.36.0 011 2.46V26.37a3.35 3.35.0 01-1 2.47 3.38 3.38.0 01-2.51 1.02zm-5.38-6.71a.79.79.0 00.85-.66L24.73 9.24a.55.55.0 00-.18-.46.62.62.0 00-.41-.17q-.08.0-16.53 6.11a.59.59.0 00-.41.59.57.57.0 00.43.52l4 1.24 1.61 4.83a.62.62.0 00.63.43.56.56.0 00.4-.17L16.54 20l4.09 3A.9.9.0 0021.11 23.15zM13.8 20.71l-1.21-4q8.72-5.55 8.78-5.55c.15.0.23.0.23.16a.18.18.0 010 .06s-2.51 2.3-7.52 6.8z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Reinforcement Learning 101 on ycombinator" href="https://news.ycombinator.com/submitlink?t=Reinforcement%20Learning%20101&u=https%3a%2f%2felkotito.github.io%2fposts%2freinforcement-learning-101%2f"><svg width="30" height="30" viewBox="0 0 512 512" fill="currentcolor" xmlns:inkscape="http://www.inkscape.org/namespaces/inkscape"><path d="M449.446.0C483.971.0 512 28.03 512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446zM183.8767 87.9921h-62.034L230.6673 292.4508V424.0079h50.6655V292.4508L390.1575 87.9921H328.1233L256 238.2489z"/></svg></a></li></ul></footer></article></main><footer class=footer><span>&copy; 2025 <a href=https://elkotito.github.io/>Mateusz Pieniak</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>